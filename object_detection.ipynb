{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pylab as pl\n",
    "from scipy import ndimage\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of visual words model\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        category = []\n",
    "        path = folder + \"/\" + filename\n",
    "        for cat in os.listdir(path):\n",
    "            img = cv2.imread(path + \"/\" + cat,0)\n",
    "            #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            if img is not None:\n",
    "                category.append(img)\n",
    "        images[filename] = category\n",
    "    return images\n",
    "# take all images category by category \n",
    "images = load_images_from_folder('./dataset')  \n",
    "# take test images \n",
    "test = load_images_from_folder(\"./query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sift features \n",
    "def sift_features(images):\n",
    "    sift_vectors = {}\n",
    "    descriptor_list = []\n",
    "    sift = cv2.SIFT_create()\n",
    "    for key,value in images.items():\n",
    "        features = []\n",
    "        for img in value:\n",
    "            kp, des = sift.detectAndCompute(img,None)\n",
    "           \n",
    "            \n",
    "            descriptor_list.extend(des)\n",
    "            features.append(des)\n",
    "        sift_vectors[key] = features\n",
    "    return [descriptor_list, sift_vectors]\n",
    "\n",
    "sifts = sift_features(images) \n",
    "# Takes the descriptor list which is unordered one\n",
    "descriptor_list = sifts[0] \n",
    "# Takes the sift features that is seperated class by class for train data\n",
    "all_bovw_feature = sifts[1] \n",
    "# Takes the sift features that is seperated class by class for test data\n",
    "test_bovw_feature = sift_features(test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A k-means clustering algorithm who takes 2 parameter which is number \n",
    "# of cluster(k) and the other is descriptors list(unordered 1d array)\n",
    "# Returns an array that holds central points.\n",
    "def kmeans(k, descriptor_list):\n",
    "    kmeans = KMeans(n_clusters = k, n_init=10)\n",
    "    kmeans.fit(descriptor_list)\n",
    "    visual_words = kmeans.cluster_centers_ \n",
    "    return visual_words\n",
    "    \n",
    "# Takes the central points which is visual words    \n",
    "visual_words = kmeans(150, descriptor_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the nearest visual word for each sift feature and return index of visual word\n",
    "def find_index(image, center):\n",
    "    count = 0\n",
    "    ind = 0\n",
    "    for i in range(len(center)):\n",
    "        if(i == 0):\n",
    "           count = distance.euclidean(image, center[i]) \n",
    "           #count = L1_dist(image, center[i])\n",
    "        else:\n",
    "            dist = distance.euclidean(image, center[i]) \n",
    "            #dist = L1_dist(image, center[i])\n",
    "            if(dist < count):\n",
    "                ind = i\n",
    "                count = dist\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2 parameters. The first one is a dictionary that holds the descriptors that are separated class by class \n",
    "# And the second parameter is an array that holds the central points (visual words) of the k means clustering\n",
    "# Returns a dictionary that holds the histograms for each images that are separated class by class. \n",
    "def image_class(all_bovw, centers):\n",
    "    dict_feature = {}\n",
    "    for key,value in all_bovw.items():\n",
    "        category = []\n",
    "        for img in value:\n",
    "            histogram = np.zeros(len(centers))\n",
    "            for each_feature in img:\n",
    "                ind = find_index(each_feature, centers)\n",
    "                histogram[ind] += 1\n",
    "            category.append(histogram)\n",
    "        dict_feature[key] = category\n",
    "    return dict_feature\n",
    "    \n",
    "# Creates histograms for train data    \n",
    "bovw_train = image_class(all_bovw_feature, visual_words) \n",
    "# Creates histograms for test data\n",
    "bovw_test = image_class(test_bovw_feature, visual_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-NN algorithm. We use this for predict the class of test images.\n",
    "# Takes 2 parameters. images is the feature vectors of train images and tests is the feature vectors of test images\n",
    "# Returns an array that holds number of test images, number of correctly predicted images and records of class based images respectively\n",
    "def knn(images, tests):\n",
    "    num_test = 0\n",
    "    correct_predict = 0\n",
    "    class_based = {}\n",
    "    \n",
    "    for test_key, test_val in tests.items():\n",
    "        # list that holds number of correctly predicted images and number of all images in a class\n",
    "        class_based[test_key] = [0, 0] \n",
    "        for tst in test_val:\n",
    "            predict_start = 0\n",
    "            \n",
    "            minimum = 0\n",
    "            key = \"a\" \n",
    "            for train_key, train_val in images.items():\n",
    "                for train in train_val:\n",
    "                    if(predict_start == 0):\n",
    "                        minimum = distance.euclidean(tst, train)\n",
    "                        key = train_key\n",
    "                        predict_start += 1\n",
    "                    else:\n",
    "                        dist = distance.euclidean(tst, train)\n",
    "                        if(dist < minimum):\n",
    "                            minimum = dist\n",
    "                            key = train_key\n",
    "            # if the predicted class is equal to the real class, increase the number of correctly predicted images\n",
    "            if(test_key == key):\n",
    "                correct_predict += 1\n",
    "                class_based[test_key][0] += 1\n",
    "            num_test += 1\n",
    "            class_based[test_key][1] += 1\n",
    "    return [num_test, correct_predict, class_based]\n",
    "    \n",
    "# Call the knn function    \n",
    "results_bowl = knn(bovw_train, bovw_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 63.366336633663366%\n",
      "\n",
      "Class based accuracies: \n",
      "\n",
      "Cat : 67.32673267326733%\n",
      "Dog : 59.4059405940594%\n"
     ]
    }
   ],
   "source": [
    "# Calculates the average accuracy and class based accuracies.  \n",
    "def accuracy(results):\n",
    "    avg_accuracy = (results[1] / results[0]) * 100\n",
    "    print(\"Average accuracy: \" + str(avg_accuracy)+\"%\")\n",
    "    print(\"\\nClass based accuracies: \\n\")\n",
    "    for key,value in results[2].items():\n",
    "        acc = (value[0] / value[1]) * 100\n",
    "        print(key + \" : \" + str(acc)+\"%\")\n",
    "        \n",
    "# Calculates the accuracies and write the results to the console.       \n",
    "accuracy(results_bowl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 402 images belonging to 2 classes.\n",
      "Found 202 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\torip\\AppData\\Local\\Temp\\ipykernel_3592\\3823620078.py:41: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(training_set, steps_per_epoch = STEPS_PER_EPOCH, epochs = EPOCHS, verbose =1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 33s 5s/step - loss: 0.8795 - accuracy: 0.5260\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 34s 6s/step - loss: 0.4700 - accuracy: 0.7812\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 34s 6s/step - loss: 0.3291 - accuracy: 0.8698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14706ba00a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing accuracy with a pretrained model\n",
    "# we will use transfer learning on a pretrianed VGG16 model for cats and dogs classification\n",
    "# import VGG 16 from keras.applications\n",
    "from tensorflow.keras.applications import VGG16\n",
    "# instantiate the model using the imagenet weights and input shape of 224x224x3\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "# Make the VGG layers non-trainable\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# input_ is VGG16 input layer\n",
    "input_ = vgg16.input\n",
    "# output_ is VGG16 output layer\n",
    "output_ = vgg16(input_)\n",
    "# Flatten the output\n",
    "last_layer = Flatten(name='flatten')(output_)\n",
    "# Add a dense layer with 1 neuron and sigmoid activation\n",
    "last_layer = Dense(1, activation='sigmoid')(last_layer)\n",
    "# Create the model with input_ as input and last_layer as output\n",
    "model = Model(inputs = input_, outputs = last_layer)\n",
    "# Define the training parameters\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = 200 // BATCH_SIZE\n",
    "EPOCHS = 3\n",
    "# Compile the model\n",
    "model.compile(optimizer ='adam',loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "# Create the training and validation generators\n",
    "training_data_generator = ImageDataGenerator(rescale = 1./255)\n",
    "testing_data_generator = ImageDataGenerator(rescale = 1./255)\n",
    "# Create the training and validation data\n",
    "training_set = training_data_generator.flow_from_directory('./dataset', target_size=(224,224),\n",
    "                                                           batch_size = BATCH_SIZE, class_mode = 'binary')\n",
    "test_set = testing_data_generator.flow_from_directory('./query',\n",
    "                                             target_size = (224, 224),\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             class_mode = 'binary')\n",
    "# Train the model\n",
    "model.fit_generator(training_set, steps_per_epoch = STEPS_PER_EPOCH, epochs = EPOCHS, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\torip\\AppData\\Local\\Temp\\ipykernel_3592\\1939184459.py:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  score = model.evaluate_generator(test_set,len(test_set))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5274196863174438\n",
      "accuracy: 0.698019802570343\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate_generator(test_set,len(test_set))\n",
    "# Print the test accuracy\n",
    "for idx, metric in enumerate(model.metrics_names):\n",
    "    print(\"{}: {}\".format(metric,score[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7281080ee6fc425e6f98dec69755e44604bddc2a93c752be7503405a79136eb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
